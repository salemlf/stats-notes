---
title: "6.5: The Method of Moment-Generating Functions"
---

import Accordion from "../../../app/components/Accordion.tsx";
import Callout from "../../../app/components/Callout.tsx";

import Ch6Ex37 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-37.mdx";
import Ch6Ex38 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-38.mdx";
import Ch6Ex39 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-39.mdx";
import Ch6Ex40 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-40.mdx";
import Ch6Ex41 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-41.mdx";
import Ch6Ex42 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-42.mdx";
import Ch6Ex43 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-43.mdx";
import Ch6Ex44 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-44.mdx";
import Ch6Ex45 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-45.mdx";
import Ch6Ex46 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-46.mdx";
import Ch6Ex47 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-47.mdx";
import Ch6Ex48 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-48.mdx";
import Ch6Ex49 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-49.mdx";
import Ch6Ex50 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-50.mdx";
import Ch6Ex51 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-51.mdx";
import Ch6Ex52 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-52.mdx";
import Ch6Ex53 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-53.mdx";
import Ch6Ex54 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-54.mdx";
import Ch6Ex55 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-55.mdx";
import Ch6Ex56 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-56.mdx";
import Ch6Ex57 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-57.mdx";
import Ch6Ex58 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-58.mdx";
import Ch6Ex59 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-59.mdx";
import Ch6Ex60 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-60.mdx";
import Ch6Ex61 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-61.mdx";
import Ch6Ex62 from "../../exercises/chapter-6-exercises/section-6-5-exercises/_exercise-6-62.mdx";

import Ch6Ex37Sol from "../../solutions/chapter-6-solutions/section-6-5-solutions/_exercise-6-37-solution.mdx";

<Callout title="Theorem 6.1" type="note">
  Let $m_X(t)$ and $m_Y(t)$ denote the moment-generating functions of random
  variables $X$ and $Y$, respectively. If both moment-generating functions exist
  and $m_X(t) = m_Y(t)$ for all values of $t$, then $X$ and $Y$ have the same
  probability distribution.
</Callout>

(The proof of Theorem 6.1 is beyond the scope of this text.)

<Callout title="Example 6.10" type="tip">
    Suppose that $Y$ is a normally distributed random variable with mean $\mu$
    variance $\sigma^2$. Show that

    $$
    Z = \frac{Y - \mu}{\sigma}
    $$

    has a _standard normal_ distribution, and normal distribution with mean $0$ and variance $1$.

    <Accordion title="Solution">
        We have seen in Example 4.16 that $Y - \mu$ has moment-generating function $e^{t^2 \sigma^2 / 2}$.
        Hence,

        $$
        m_Z(t) = E \left(e^{tZ} \right) = E \left[e^{(t / \sigma) (Y - \mu)} \right] = m_{Y - \mu} \left(\frac{t}{\sigma} \right) = e^{(t / \sigma)^2 (\sigma^2 / 2)} = e^{t^2 / 2}.
        $$

        On comparing $m_Z(t)$ with the moment-generating function of a normal random variable, we see that $Z$ must be normally distributed with $E(Z) = 0$ and $V(Z) = 1$.
    </Accordion>

</Callout>

<br></br>

<Callout title="Example 6.11" type="tip">
  Let $Z$ be a normally distributed random variable with mean $0$ and variance
  $1$. Use the method of moment-generating functions to ﬁnd the probability
  distribution of $Z^2$.

  <Accordion title="Solution">
    The moment-generating function for $Z^2$ is

    $$
    \begin{align*}
        m_{Z^2}(t) = E \left(e^{tZ^2} \right) & = \int_{-\infty}^{\infty} e^{tz^2} f(z) dz = \int_{-\infty}^{\infty} e^{tz^2} \frac{e^{-z^2 / 2}}{\sqrt{2 \pi}} dz\\
        & = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-(z^2 / 2)(1 - 2t)} dz.
    \end{align*}
    $$

    This integral can be evaluated either by consulting a table of integrals or by noting that, if $1 - 2t > 0$ (equivalently, $t < 1 / 2$), the integrand

    $$
    \frac{\exp \left[-\left(\frac{z^2}{2} \right) (1 - 2t) \right]}{\sqrt{2 \pi}} = \frac{\exp \left[-\left(\frac{z^2}{2} \right) / (1 - 2t)^{-1} \right]}{\sqrt{2 \pi}}
    $$

    is proportional to the density function of a normally distributed random variable with mean $0$ and variance $(1 - 2t)^{-1}$.
    To make the integrand a normal density function (so that the deﬁnite integral is equal to $1$), multiply the numerator and denominator by the standard deviation, $(1 - 2t)^{-1 / 2}$.
    Then

    $$
    m_{Z^2}(t) = \frac{1}{(1 - 2t)^{1 / 2}} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi} (1 - 2t)^{-1 / 2}} \exp \left[-\left(\frac{z^2}{2} \right) / (1 - 2t)^{-1} \right] dz.
    $$

    Because the integral equals $1$, if $t < 1 / 2$,

    $$
    m_{Z^2}(t) = \frac{1}{(1 - 2t)^{1 / 2}} = (1 - 2t)^{-1 / 2}.
    $$

    A comparison of $m_{Z^2}(t)$ with the moment-generating functions in Appendix 2 shows that $m_{Z^2}(t)$ is identical to the moment-generating function for the gamma-distributed random variable with $\alpha = 1 / 2$ and $\beta = 2$.
    Thus, using Definition 4.10, $Z^2$ has a $\chi^2$ distribution with $\nu = 1$ degree of freedom.
    It follows that the density function for $U = Z^2$ is given by

    $$
    f_U(u) =
    \begin{cases}
        \frac{u^{-1 / 2} e^{-u / 2}}{\Gamma \left(\frac{1}{2} \right) 2^{1 / 2}}, & u \geq 0,\\
        0, & \text{elsewhere.}
    \end{cases}
    $$

  </Accordion>
</Callout>

<br></br>

<Callout title="Theorem 6.2" type="note" id="thm-6-2">
    Let $Y_1, Y_2, ..., Y_n$ be independent random variables with
    moment-generating functions $m_{Y_1}(t), m_{Y_2}(t), ..., m_{Y_n}(t)$,
    respectively. If $U = Y_1 + Y_2 + ... + Y_n$, then
  
    $$
    m_{U}(t) = m_{Y_1}(t) \times m_{Y_2}(t) \times ... \times m_{Y_n}(t).
    $$

    <Accordion title="Proof">
        We know that, because the random variables $Y_1, Y_2, ..., Y_n$ are
        independent (see Theorem 5.9),

        $$
        \begin{align*}
            m_{U}(t) & = E \left[e^{t(Y_1 + Y_2 + ... + Y_n)} \right] = E \left(e^{tY_1} e^{tY_2} \cdots e^{tY_n} \right)\\
            & = E \left(e^{tY_1} \right) \times E \left(e^{tY_2} \right) \times \cdots \times E \left(e^{tY_n} \right).
        \end{align*}
        $$

        Thus, by the definition of moment-generating functions,

        $$
        m_U(t) = m_{Y_1}(t) \times m_{Y_2}(t) \times \cdots \times m_{Y_n}(t).
        $$
    </Accordion>

</Callout>

<br></br>

<Callout title="Theorem 6.3" type="note">
    Let $Y_1, Y_2, ..., Y_n$ be independent normally distributed random variables
    with $E(Y_i) = \mu_i$ and $V(Y_i) = \sigma_i^2$, for $i = 1, 2, ..., n$, and
    let $a_1, a_2, ..., a_n$ be constants. If

    $$
    U = \sum_{i = 1}^n a_i Y_i = a_1 Y_1 + a_2 Y_2 + ... + a_n Y_n,
    $$

    then $U$ is a normally distributed random variable with

    $$
    E(U) = \sum_{i = 1}^n a_i \mu_i = a_1 \mu_1 + a_2 \mu_2 + ... + a_n \mu_n
    $$

    and

    $$
    V(U) = \sum_{i = 1}^n a_i \sigma_i^2 = a_1 \sigma_1^2 + a_2 \sigma_2^2 + ... + a_n \sigma_n^2.
    $$

    <Accordion title="Proof">
        Because $Y_i$ is normally distributed with mean $\mu_i$ and variance $\sigma_i^2$, $Y_i$ has moment-generating function given by

        $$
        m_{Y_i}(t) = \exp \left(\mu_i t + \frac{\sigma_i^2 t^2}{2} \right).
        $$

        [Recall that $\exp(\cdot)$ is a more convenient way to write $e^{(\cdot)}$ when the term in the exponent is long or complex.]
        Therefore, $a_i Y_i$ has moment-generating function given by

        $$
        m_{a_i Y_i}(t) = E \left(e^{ta_i Y_i} \right) = m_{Y_i}(a_i t) = \exp \left(\mu_i a_i t + \frac{a_i^2 \sigma_i^2 t^2}{2} \right).
        $$

        Because the random variables $Y_i$ are independent, the random variables $a_i Y_i$ are independent, for $i = 1, 2, ..., n$, and Theorem 6.2 implies that

        $$
        \begin{align*}
            m_{U}(t) & = m_{a_1 Y_1}(t) \times m_{a_2 Y_2}(t) \times \cdots \times m_{a_n Y_n}(t)\\
            & = \exp \left(\mu_1 a_1 t + \frac{a_1^2 \sigma_1^2 t^2}{2} \right) \times \cdots \times \exp \left(\mu_n a_n t + \frac{a_n^2 \sigma_n^2 t^2}{2} \right)\\
            & = \exp \left(t \sum_{i = 1}^n a_i \mu_i + \frac{t^2}{2} \sum_{i = 1}^n a_i^2 \sigma_i^2 \right).
        \end{align*}
        $$

        Thus, $U$ has a normal distribution with mean $\sum_{i = 1}^n a_i \mu_i$ and variance $\sum_{i = 1}^n a_i^2 \sigma_i^2$.
    </Accordion>

</Callout>

<br></br>

<Callout title="Theorem 6.4" type="note">
    Let $Y_1, Y_2, ..., Y_n$ be defined as in Theorem 6.3 and defined $Z_i$ by

    $$
    Z_i = \frac{Y_i - \mu_i}{\sigma_i}, \quad i = 1, 2, ..., n.
    $$

    Then $\sum_{i = 1}^n Z_i^2$ has a $\chi^2$ distribution with $n$ degrees of freedom.

    <Accordion title="Proof">
        Because $Y_i$ is normally distributed with mean $\mu_i$ and variance $\sigma_i^2$, the result of Example 6.10 implies that $Z_i$ is normally distributed with mean $0$ and variance $1$.
        From Example 6.11, we then have that $Z_i^2$ is a $\chi^2$-distributed random variable with 1 degree of freedom.
        Thus,

        $$
        m_{Z_i^2}(t) = (1 - 2t)^{-1 / 2},
        $$

        and from Theorem 6.2, with $V = \sum_{i = 1}^n Z_i^2$,

        $$
        \begin{align*}
            m_V(t) & = m_{Z_1^2}(t) \times m_{Z_2^2}(t) \times \cdots \times m_{Z_n^2}(t)\\
            & = (1 - 2t)^{-1 / 2} \times (1 - 2t)^{-1 / 2} \times \cdots \times (1 - 2t)^{-1 / 2} = (1 - 2t)^{-n / 2}.
        \end{align*}
        $$

        Because moment-generating functions are unique, $V$ has a $\chi^2$ distribution with $n$ degrees of freedom.
    </Accordion>

</Callout>

<br></br>

<Callout title="Summary of the Moment-Generating Function Method" type="info">
  Let $U$ be a function of the random variables $Y_1, Y_2, ..., Y_n$.

  <ol>
    <li>Find the moment-generating function for $U$, $m_U(t)$.</li>
    <li>
        Compare $m_U(t)$ with other well-known moment-generating functions.
        If $m_U(t) = m_V(t)$ for all values of $t$, Theorem 6.1 implies that $U$ and $V$ have identical distributions.
    </li>
  </ol>
</Callout>

## <u>Exercises</u>

### <u>6.37</u>

<Ch6Ex37 />

<Accordion title="Proof">
  <Ch6Ex37Sol />
</Accordion>

### <u>6.38</u>

<Ch6Ex38 />

### <u>6.39</u>

<Ch6Ex39 />

### <u>6.40</u>

<Ch6Ex40 />

### <u>6.41</u>

<Ch6Ex41 />

### <u>6.42</u>

<Ch6Ex42 />

### <u>6.43</u>

<Ch6Ex43 />

### <u>6.44</u>

<Ch6Ex44 />

### <u>6.45</u>

<Ch6Ex45 />

### <u>6.46</u>

<Ch6Ex46 />

### <u>6.47</u>

<Ch6Ex47 />

### <u>6.48</u>

<Ch6Ex48 />

### <u>6.49</u>

<Ch6Ex49 />

### <u>6.50</u>

<Ch6Ex50 />

### <u>6.51</u>

<Ch6Ex51 />

### <u>6.52</u>

<Ch6Ex52 />

### <u>6.53</u>

<Ch6Ex53 />

### <u>6.54</u>

<Ch6Ex54 />

### <u>6.55</u>

<Ch6Ex55 />

### <u>6.56</u>

<Ch6Ex56 />

### <u>6.57</u>

<Ch6Ex57 />

### <u>6.58</u>

<Ch6Ex58 />

### <u>6.59</u>

<Ch6Ex59 />

### <u>6.60</u>

<Ch6Ex60 />

### <u>6.61</u>

<Ch6Ex61 />

### <u>6.62</u>

<Ch6Ex62 />
